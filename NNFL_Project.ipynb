{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NNFL_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG9pvvzK7TaI"
      },
      "source": [
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import sys\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from nltk import ngrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "from collections import Counter\n",
        "import editdistance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTgDn0Cb7dNl",
        "outputId": "77c66ae9-96e6-4a36-be48-68ac52b4993b"
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "CUDA #Boolean variable to check the presence of CUDA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuNkd1N35on9"
      },
      "source": [
        "class CorpusSearcher(object):\n",
        "    \"\"\"\n",
        "      To create corpuses and retreive attributes closest to the given query.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, query_corpus, key_corpus, value_corpus, vectorizer, make_binary=True):\n",
        "        self.vectorizer = vectorizer\n",
        "        self.vectorizer.fit(key_corpus)\n",
        "\n",
        "        self.query_corpus = query_corpus\n",
        "        self.key_corpus = key_corpus\n",
        "        self.value_corpus = value_corpus\n",
        "        \n",
        "        # rows = docs, cols = features\n",
        "        self.key_corpus_matrix = self.vectorizer.transform(key_corpus)\n",
        "        if make_binary:\n",
        "            # make binary\n",
        "            self.key_corpus_matrix = (self.key_corpus_matrix != 0).astype(int)\n",
        "\n",
        "        \n",
        "    def most_similar(self, key_idx, n=10):\n",
        "        \"\"\" \n",
        "          Score the query against the key corpus and return the values corresponding to the \n",
        "          top N scores from the value corpus.\n",
        "          Used for retrieving attributes from sentences with similar content to the query sentence.\n",
        "\n",
        "          Parameter:\n",
        "          - key_idx : id of the attribute in the query corpus\n",
        "          - n : number of closest attributes to be returned\n",
        "\n",
        "          Returns:\n",
        "          - selected : list of closest N values in the value corpus with their index and score \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        query = self.query_corpus[key_idx]\n",
        "        query_vec = self.vectorizer.transform([query])\n",
        "\n",
        "        scores = np.dot(self.key_corpus_matrix, query_vec.T)\n",
        "        scores = np.squeeze(scores.toarray())\n",
        "        scores_indices = zip(scores, range(len(scores)))\n",
        "        selected = sorted(scores_indices, reverse=True)[:n]\n",
        "\n",
        "        # use the retrieved index 'i' to pick examples from the VALUE corpus\n",
        "        selected = [ (self.value_corpus[i], i, score) for (score, i) in selected ]\n",
        "\n",
        "        return selected"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms6f2HI9QLOI"
      },
      "source": [
        "def build_vocab_maps(vocab_file):\n",
        "    \"\"\"\n",
        "      Creates and returns two dictionaries, one to map vocabulary words to unique ids and \n",
        "      one to map the unique ids back to the vocabulary words.\n",
        "      \n",
        "      Parameters: \n",
        "      - vocab_file : os path to vocabulary file\n",
        "      \n",
        "      Returns:\n",
        "      - tok_to_id : dictionary which fetches id from token key\n",
        "      - id_to_tok : dictionary which fetches token from id key\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    assert os.path.exists(vocab_file), \"The vocab file %s does not exist\" % vocab_file\n",
        "    unk = '<unk>'\n",
        "    pad = '<pad>'\n",
        "    sos = '<s>'\n",
        "    eos = '</s>'\n",
        "\n",
        "    lines = [x.strip() for x in open(vocab_file)]\n",
        "\n",
        "    assert lines[0] == unk and lines[1] == pad and lines[2] == sos and lines[3] == eos, \\\n",
        "        \"The first words in %s are not %s, %s, %s, %s\" % (vocab_file, unk, pad, sos, eos)\n",
        "\n",
        "    tok_to_id = {}\n",
        "    id_to_tok = {}\n",
        "    for i, vi in enumerate(lines):\n",
        "        tok_to_id[vi] = i\n",
        "        id_to_tok[i] = vi\n",
        "\n",
        "    # appending an extra vocab item for empty attribute lines\n",
        "    empty_tok_idx =  len(id_to_tok)\n",
        "    tok_to_id['<empty>'] = empty_tok_idx\n",
        "    id_to_tok[empty_tok_idx] = '<empty>'\n",
        "\n",
        "    return tok_to_id, id_to_tok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvJVZOkQZf5"
      },
      "source": [
        "def extract_attributes(line, attribute_vocab, use_ngrams=False):\n",
        "    \"\"\"\n",
        "      Split the given sentence into its attribute markers and attribute-independent content.\n",
        "      This is the 'DELETE' process in the paper\n",
        "\n",
        "      Parameters:\n",
        "      - line: the given sentence\n",
        "      - attribute_vocab: the complete vocabulary of attributes\n",
        "      - use_ngrams: boolean, True if ngrams of the sentence should be checked instead of individual words\n",
        "\n",
        "      Returns:\n",
        "      - line\n",
        "      - content : attribute-independent content remaining after attributes are deleted\n",
        "      - attribute_markers : list of attributes from the given sentence\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if use_ngrams:\n",
        "        # generate all ngrams for the sentence\n",
        "        grams = []\n",
        "        for i in range(1, 5):\n",
        "            try:\n",
        "                i_grams = [ \" \".join(gram) for gram in ngrams(line, i) ]\n",
        "                grams.extend(i_grams)\n",
        "            except RuntimeError:\n",
        "                continue\n",
        "\n",
        "        # filter ngrams by whether they appear in the attribute_vocab\n",
        "        candidate_markers = [ (gram, attribute_vocab[gram]) for gram in grams if gram in attribute_vocab ]\n",
        "\n",
        "        # sort attribute markers by score and prepare for 'deletion'\n",
        "        content = \" \".join(line)\n",
        "        candidate_markers.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        candidate_markers = [marker for (marker, score) in candidate_markers]\n",
        "        \n",
        "        # seperate attributes and attribute-independent content\n",
        "        attribute_markers = []\n",
        "        for marker in candidate_markers:\n",
        "            if marker in content:\n",
        "                attribute_markers.append(marker)\n",
        "                content = content.replace(marker, \"\")\n",
        "        content = content.split()\n",
        "        \n",
        "    else:\n",
        "        # same thing, but without the use of ngrams\n",
        "        content = []\n",
        "        attribute_markers = []\n",
        "        for tok in line:\n",
        "            if tok in attribute_vocab:\n",
        "                attribute_markers.append(tok)\n",
        "            else:\n",
        "                content.append(tok)\n",
        "\n",
        "    return line, content, attribute_markers\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdPafe7jQf8J"
      },
      "source": [
        "def read_nmt_data(src, config, tgt, attribute_vocab, train_src=None, train_tgt=None, ngram_attributes=False):\n",
        "    \"\"\"\n",
        "      Initializer function to read data from files and store it in 'src' and 'tgt'\n",
        "\n",
        "      Parameters:\n",
        "      - src, tgt : os paths to files containging source and target sentences respectively\n",
        "      - config : contains path to vocab files\n",
        "      - attribute_vocab : path to attribute vocabulary \n",
        "      - train_src, train_tgt : \n",
        "      - ngram_attributes : boolean, if True then attributes are ngrams instead of direct mappings\n",
        "\n",
        "      Returns:\n",
        "      - src, tgt: dictionaries containing source and target date respectively\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    if ngram_attributes:\n",
        "        # read attribute vocab as a dictionary mapping attributes to scores\n",
        "        pre_attr = {}\n",
        "        post_attr = {}\n",
        "        with open(attribute_vocab) as attr_file:\n",
        "            next(attr_file) # skip header\n",
        "            for line in attr_file:\n",
        "                parts = line.strip().split()\n",
        "                pre_salience = float(parts[-2])\n",
        "                post_salience = float(parts[-1])\n",
        "                attr = ' '.join(parts[:-2])\n",
        "                pre_attr[attr] = pre_salience\n",
        "                post_attr[attr] = post_salience\n",
        "    else:\n",
        "        pre_attr = post_attr = set([x.strip() for x in open(attribute_vocab)])\n",
        "\n",
        "    src_lines = [l.strip().lower().split() for l in open(src, 'r')]\n",
        "    src_lines, src_content, src_attribute = list(zip(*[extract_attributes(line, pre_attr, pre_attr) for line in src_lines]))\n",
        "    src_tok2id, src_id2tok = build_vocab_maps(config['data']['src_vocab'])\n",
        "\n",
        "    # during train time, just pick attributes that are close to the current (using word distance)\n",
        "    # we don't need to do the TFIDF thing with the source because test is strictly in the src => tgt direction. \n",
        "    # But we still measure both src and tgt dist because training is bidirectional\n",
        "    # (i.e., we're autoencoding src and tgt sentences during training)\n",
        "\n",
        "    src_dist_measurer = CorpusSearcher(\n",
        "        query_corpus=[' '.join(x) for x in src_attribute],\n",
        "        key_corpus=[' '.join(x) for x in src_attribute],\n",
        "        value_corpus=[' '.join(x) for x in src_attribute],\n",
        "        vectorizer=CountVectorizer(vocabulary=src_tok2id),\n",
        "        make_binary=True\n",
        "    )\n",
        "    src = {\n",
        "        'data': src_lines, 'content': src_content, 'attribute': src_attribute,\n",
        "        'tok2id': src_tok2id, 'id2tok': src_id2tok, 'dist_measurer': src_dist_measurer\n",
        "    }\n",
        "\n",
        "    tgt_lines = [l.strip().lower().split() for l in open(tgt, 'r')] if tgt else None\n",
        "    tgt_lines, tgt_content, tgt_attribute = list(zip(*[extract_attributes(line, post_attr, post_attr) for line in tgt_lines]))\n",
        "    tgt_tok2id, tgt_id2tok = build_vocab_maps(config['data']['tgt_vocab'])\n",
        "\n",
        "    # during train time, just pick attributes that are close to the current (using word distance)\n",
        "    # since this is only used to noise the inputs\n",
        "\n",
        "    if train_src is None or train_tgt is None:\n",
        "        tgt_dist_measurer = CorpusSearcher(\n",
        "            query_corpus=[' '.join(x) for x in tgt_attribute],\n",
        "            key_corpus=[' '.join(x) for x in tgt_attribute],\n",
        "            value_corpus=[' '.join(x) for x in tgt_attribute],\n",
        "            vectorizer=CountVectorizer(vocabulary=tgt_tok2id),\n",
        "            make_binary=True\n",
        "        )\n",
        "\n",
        "    # during test time, scan through train content (using tfidf) and retrieve corresponding attributes\n",
        "    \n",
        "    else:\n",
        "        tgt_dist_measurer = CorpusSearcher(\n",
        "            query_corpus=[' '.join(x) for x in src_content],\n",
        "            key_corpus=[' '.join(x) for x in train_tgt['content']],\n",
        "            value_corpus=[' '.join(x) for x in train_tgt['attribute']],\n",
        "            vectorizer=TfidfVectorizer(vocabulary=tgt_tok2id),\n",
        "            make_binary=False\n",
        "        )\n",
        "    tgt = {\n",
        "        'data': tgt_lines, 'content': tgt_content, 'attribute': tgt_attribute,\n",
        "        'tok2id': tgt_tok2id, 'id2tok': tgt_id2tok, 'dist_measurer': tgt_dist_measurer\n",
        "    }\n",
        "\n",
        "    return src, tgt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShRAkPb_QmpZ"
      },
      "source": [
        "def sample_replace(lines, dist_measurer, sample_rate, corpus_idx):\n",
        "    \"\"\"\n",
        "      Replace sample_rate * batch_size lines with nearby examples (according to dist_measurer).\n",
        "      This is not exactly the same as the paper (words are shared during train) but its essentially the same idea and easier to implement.\n",
        "\n",
        "      Parameters:\n",
        "      - lines : list of sentences\n",
        "      - dist_measurer : object of CorpusSearcher\n",
        "      - sample_rate : percentage of samples to be replaced with examples close to given one\n",
        "      - corpus_idx : given sample\n",
        "\n",
        "      Returns:\n",
        "      - out: list of sentences after changing\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    out = [None for _ in range(len(lines))]\n",
        "    for i, line in enumerate(lines):\n",
        "        if random.random() < sample_rate:\n",
        "            # ignore first line since top match is the current line\n",
        "            sims = dist_measurer.most_similar(corpus_idx + i)[1:]\n",
        "            \n",
        "            try:\n",
        "                line = next( (\n",
        "                    tgt_attr.split() for tgt_attr, _, _ in sims\n",
        "                    if set(tgt_attr.split()) != set(line[1:-1]) # and tgt_attr != ''\n",
        "                ) )\n",
        "            # all the matches are blanks\n",
        "            except StopIteration:\n",
        "                line = []\n",
        "            line = ['<s>'] + line + ['</s>']\n",
        "\n",
        "        # special empty token for empty sequences (just start/end tok)\n",
        "        if len(line) == 2:\n",
        "            line.insert(1, '<empty>')\n",
        "        out[i] = line\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VaO5E9bQwrZ"
      },
      "source": [
        "def get_minibatch(lines, tok2id, index, batch_size, max_len, sort=False, idx=None, dist_measurer=None, sample_rate=0.0):\n",
        "    \"\"\"\n",
        "      To prepare minibatch. No sorting since we care about the order of outputs. Also acts as a helper function to implement 'Retrieve' \n",
        "\n",
        "      Parameters:\n",
        "      - lines : list of sentences\n",
        "      - tok2id : dictionary which fetches id from token key\n",
        "      - index : list of indices for which output is wanted\n",
        "      - batch_size : size of the minibatch\n",
        "      - max_len : maximum allowed length for sentences\n",
        "      - sort : boolean, False by default\n",
        "      - idx : optional; if passed the fn will only return values for those ids\n",
        "      - dist_measurer : distance measuring function, used for sample_replace\n",
        "      - sample rate : between 0 and 1\n",
        "\n",
        "      Returns:\n",
        "      - input_lines : tokenized input lines\n",
        "      - output_lines : tokenized output lines\n",
        "      - lens : list of lengths of sentences\n",
        "      - mask : 0/1 masking for each sentence\n",
        "      - idx : idx\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    lines = [\n",
        "        ['<s>'] + line[:max_len] + ['</s>']\n",
        "        for line in lines[index:index + batch_size]\n",
        "    ]\n",
        "\n",
        "    if dist_measurer is not None:\n",
        "        lines = sample_replace(lines, dist_measurer, sample_rate, index)\n",
        "\n",
        "    lens = [len(line) - 1 for line in lines]\n",
        "    max_len = max(lens)\n",
        "\n",
        "    unk_id = tok2id['<unk>']\n",
        "    input_lines = [\n",
        "        [tok2id.get(w, unk_id) for w in line[:-1]] +\n",
        "        [tok2id['<pad>']] * (max_len - len(line) + 1)\n",
        "        for line in lines\n",
        "    ]\n",
        "\n",
        "    output_lines = [\n",
        "        [tok2id.get(w, unk_id) for w in line[1:]] +\n",
        "        [tok2id['<pad>']] * (max_len - len(line) + 1)\n",
        "        for line in lines\n",
        "    ]\n",
        "\n",
        "    mask = [\n",
        "        ([1] * l) + ([0] * (max_len - l))\n",
        "        for l in lens\n",
        "    ]\n",
        "\n",
        "    if sort:\n",
        "        idx = [x[0] for x in sorted(enumerate(lens), key=lambda x: -x[1])]\n",
        "\n",
        "    if idx is not None:\n",
        "        lens = [lens[j] for j in idx]\n",
        "        input_lines = [input_lines[j] for j in idx]\n",
        "        output_lines = [output_lines[j] for j in idx]\n",
        "        mask = [mask[j] for j in idx]\n",
        "\n",
        "    input_lines = Variable(torch.LongTensor(input_lines))\n",
        "    output_lines = Variable(torch.LongTensor(output_lines))\n",
        "    mask = Variable(torch.FloatTensor(mask))\n",
        "\n",
        "    if CUDA:\n",
        "        input_lines = input_lines.cuda()\n",
        "        output_lines = output_lines.cuda()\n",
        "        mask = mask.cuda()\n",
        "\n",
        "    return input_lines, output_lines, lens, mask, idx\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAcs3m0BQ1CG"
      },
      "source": [
        "def minibatch(src, tgt, idx, batch_size, max_len, model_type, is_test=False):\n",
        "    \"\"\"\n",
        "      Fetches the inputs, outputs and attributes using get_minibatch depending on model type.\n",
        "\n",
        "      Parameters:\n",
        "      - src, tgt : source and target data dictionaries\n",
        "      - idx : list of indices for which output is wanted\n",
        "      - batch_size : size of the minibatch\n",
        "      - max_len : maximum allowed length for each sentence\n",
        "      - model_type: determines which model is followed:\n",
        "          - 'delete' for DeleleOnly\n",
        "          - 'delete_retrieve' for DeleteAndRetrieve\n",
        "          - 'seq2seq' for TemplateBased\n",
        "\n",
        "      Returns:\n",
        "      - inputs : input_lines from get_minibatch\n",
        "      - outputs : output_lines from get_minibatch\n",
        "      - attributes : attributes generated\n",
        "\n",
        "    \"\"\"\n",
        "    if not is_test:\n",
        "        use_src = random.random() < 0.5\n",
        "        in_dataset = src if use_src else tgt\n",
        "        out_dataset = in_dataset\n",
        "        attribute_id = 0 if use_src else 1\n",
        "    else:\n",
        "        in_dataset = src\n",
        "        out_dataset = tgt\n",
        "        attribute_id = 1\n",
        "\n",
        "    if model_type == 'delete':\n",
        "        inputs = get_minibatch(in_dataset['content'], in_dataset['tok2id'], idx, batch_size, max_len, sort=True)\n",
        "        outputs = get_minibatch(out_dataset['data'], out_dataset['tok2id'], idx, batch_size, max_len, idx=inputs[-1])\n",
        "\n",
        "        # since true length could be less than batch_size at end of data\n",
        "        batch_len = len(outputs[0])\n",
        "        attribute_ids = [attribute_id for _ in range(batch_len)]\n",
        "        attribute_ids = Variable(torch.LongTensor(attribute_ids))\n",
        "        if CUDA:\n",
        "            attribute_ids = attribute_ids.cuda()\n",
        "\n",
        "        attributes = (attribute_ids, None, None, None, None)\n",
        "\n",
        "    elif model_type == 'delete_retrieve':\n",
        "        inputs =  get_minibatch(in_dataset['content'], in_dataset['tok2id'], idx, batch_size, max_len, sort=True)\n",
        "        outputs = get_minibatch(out_dataset['data'], out_dataset['tok2id'], idx, batch_size, max_len, idx=inputs[-1])\n",
        "\n",
        "        if is_test:\n",
        "            # This dist_measurer has sentence attributes for values, so setting \n",
        "            # the sample rate to 1 means the output is always replaced with an\n",
        "            # attribute. So we're still getting attributes even though\n",
        "            # the method is being fed content. \n",
        "            attributes =  get_minibatch(\n",
        "                in_dataset['content'], out_dataset['tok2id'], idx, \n",
        "                batch_size, max_len, idx=inputs[-1],\n",
        "                dist_measurer=out_dataset['dist_measurer'], sample_rate=1.0)\n",
        "        else:\n",
        "            attributes =  get_minibatch(\n",
        "                out_dataset['attribute'], out_dataset['tok2id'], idx, \n",
        "                batch_size, max_len, idx=inputs[-1],\n",
        "                dist_measurer=out_dataset['dist_measurer'], sample_rate=0.1)\n",
        "            \n",
        "        attributes = (None, None, None, None, None)\n",
        "\n",
        "    else:\n",
        "        raise Exception('Unsupported model_type: %s' % model_type)\n",
        "\n",
        "    return inputs, attributes, outputs\n",
        "\n",
        "\n",
        "def unsort(arr, idx):\n",
        "    \"\"\"\n",
        "      Unsort a list given a list of each element's original index\n",
        "    \"\"\"\n",
        "    unsorted_arr = arr[:]\n",
        "    for i, origin in enumerate(idx):\n",
        "        unsorted_arr[origin] = arr[i]\n",
        "    return unsorted_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeI-ke1V73g3"
      },
      "source": [
        "#Encoder\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\" \n",
        "      Bi-directional LSTM to encode sentence+attributes.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, emb_dim, hidden_dim, layers, bidirectional, dropout, pack=True):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.num_directions = 2 if bidirectional else 1\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim,\n",
        "            hidden_dim // self.num_directions,\n",
        "            layers,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True,\n",
        "            dropout=dropout)\n",
        "\n",
        "        self.pack = pack\n",
        "\n",
        "    def init_state(self, input):\n",
        "        batch_size = input.size(0) \n",
        "        h0 = Variable(torch.zeros(\n",
        "            self.lstm.num_layers * self.num_directions,\n",
        "            batch_size,\n",
        "            self.lstm.hidden_size\n",
        "        ), requires_grad=False)\n",
        "        c0 = Variable(torch.zeros(\n",
        "            self.lstm.num_layers * self.num_directions,\n",
        "            batch_size,\n",
        "            self.lstm.hidden_size\n",
        "        ), requires_grad=False)\n",
        "\n",
        "        if CUDA:\n",
        "            return h0.cuda(), c0.cuda()\n",
        "        else:\n",
        "            return h0, c0\n",
        "\n",
        "\n",
        "    def forward(self, src_embedding, srclens, srcmask, temp=1):\n",
        "        h0, c0 = self.init_state(src_embedding)\n",
        "\n",
        "        if self.pack:\n",
        "            inputs = pack_padded_sequence(src_embedding, srclens, batch_first=True)\n",
        "        else:\n",
        "            inputs = src_embedding\n",
        "\n",
        "        outputs, (h_final, c_final) = self.lstm(inputs, (h0, c0))\n",
        "\n",
        "        if self.pack:\n",
        "            outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "\n",
        "        return outputs, (h_final, c_final)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRaB2rLK8Ql8"
      },
      "source": [
        "#Decoders\n",
        "class BilinearAttention(nn.Module):\n",
        "    \"\"\" \n",
        "      Bilinear attention layer: score(H_j, q) = H_j^T W_a q (where W_a = self.in_projection)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden):\n",
        "        super(BilinearAttention, self).__init__()\n",
        "        self.in_projection = nn.Linear(hidden, hidden, bias=False)\n",
        "        self.softmax = nn.Softmax()\n",
        "        self.out_projection = nn.Linear(hidden * 2, hidden, bias=False)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, query, keys, srcmask=None, values=None):\n",
        "        \"\"\"\n",
        "            query: [batch, hidden]\n",
        "            keys: [batch, len, hidden]\n",
        "            values: [batch, len, hidden] (optional, if none will = keys)\n",
        "\n",
        "            compare query to keys, use the scores to find weighted sum of values\n",
        "            if no value is specified, then values = keys\n",
        "        \"\"\"\n",
        "        \n",
        "        if values is None:\n",
        "            values = keys\n",
        "    \n",
        "        # [Batch, Hidden, 1]\n",
        "        decoder_hidden = self.in_projection(query).unsqueeze(2)\n",
        "        # [Batch, Source length]\n",
        "        attn_scores = torch.bmm(keys, decoder_hidden).squeeze(2)\n",
        "        if srcmask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(srcmask, -float('inf'))\n",
        "            \n",
        "        attn_probs = self.softmax(attn_scores)\n",
        "        # [Batch, 1, source length]\n",
        "        attn_probs_transposed = attn_probs.unsqueeze(1)\n",
        "        # [Batch, hidden]\n",
        "        weighted_context = torch.bmm(attn_probs_transposed, values).squeeze(1)\n",
        "\n",
        "        context_query_mixed = torch.cat((weighted_context, query), 1)\n",
        "        context_query_mixed = self.tanh(self.out_projection(context_query_mixed))\n",
        "\n",
        "        return weighted_context, context_query_mixed, attn_probs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnJBb5oWRBxf"
      },
      "source": [
        "class AttentionalLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "      A LSTM cell with attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, config, attention):\n",
        "        super(AttentionalLSTM, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = 1\n",
        "        self.use_attention = attention\n",
        "        self.config = config\n",
        "        self.cell = nn.LSTMCell(input_dim, hidden_dim)\n",
        "\n",
        "        if self.use_attention:\n",
        "            self.attention_layer = BilinearAttention(hidden_dim)\n",
        "\n",
        "\n",
        "    def forward(self, input, hidden, ctx, srcmask, kb=None):\n",
        "        input = input.transpose(0, 1)\n",
        "\n",
        "        output = []\n",
        "        timesteps = range(input.size(0))\n",
        "        for i in timesteps:\n",
        "            hy, cy = self.cell(input[i], hidden)\n",
        "            if self.use_attention:\n",
        "                _, h_tilde, alpha = self.attention_layer(hy, ctx, srcmask)\n",
        "                hidden = h_tilde, cy\n",
        "                output.append(h_tilde)\n",
        "            else: \n",
        "                hidden = hy, cy\n",
        "                output.append(hy)\n",
        "\n",
        "        # combine outputs, and get into [time, batch, dim]\n",
        "        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n",
        "\n",
        "        output = output.transpose(0, 1)\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIOT5VhDRF-o"
      },
      "source": [
        "class StackedAttentionLSTM(nn.Module):\n",
        "    \"\"\" \n",
        "      Stacked LSTM with input feeding.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cell_class=AttentionalLSTM, config=None):\n",
        "        super(StackedAttentionLSTM, self).__init__()\n",
        "        self.options=config['model']\n",
        "\n",
        "        self.dropout = nn.Dropout(self.options['dropout'])\n",
        "\n",
        "        self.layers = []\n",
        "        input_dim = self.options['emb_dim']\n",
        "        hidden_dim = self.options['tgt_hidden_dim']\n",
        "        for i in range(self.options['tgt_layers']):\n",
        "            layer = cell_class(input_dim, hidden_dim, config, config['model']['attention'])\n",
        "            self.add_module('layer_%d' % i, layer)\n",
        "            self.layers.append(layer)\n",
        "            input_dim = hidden_dim\n",
        "\n",
        "    def forward(self, input, hidden, ctx, srcmask, kb=None):\n",
        "        h_final, c_final = [], []\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            output, (h_final_i, c_final_i) = layer(input, hidden, ctx, srcmask, kb)\n",
        "\n",
        "            input = output\n",
        "\n",
        "            if i != len(self.layers):\n",
        "                input = self.dropout(input)\n",
        "\n",
        "            h_final.append(h_final_i)\n",
        "            c_final.append(c_final_i)\n",
        "\n",
        "        h_final = torch.stack(h_final)\n",
        "        c_final = torch.stack(c_final)\n",
        "\n",
        "        return input, (h_final, c_final)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3V1UHHV8l8z"
      },
      "source": [
        "def get_latest_ckpt(ckpt_dir):\n",
        "    \"\"\"\n",
        "      Fetch latest checkpoint.\n",
        "    \"\"\"\n",
        "    ckpts = glob.glob(os.path.join(ckpt_dir, '*.ckpt'))\n",
        "    # if no checkpoints are found, continue with fresh parameters\n",
        "    if len(ckpts) == 0:\n",
        "        return -1, None\n",
        "    ckpts = map(lambda ckpt: (int(ckpt.split('.')[1]), ckpt), ckpts)\n",
        "    # get most recent checkpoint\n",
        "    epoch, ckpt_path = sorted(ckpts)[-1]\n",
        "    return epoch, ckpt_path\n",
        "\n",
        "\n",
        "def attempt_load_model(model, checkpoint_dir=None, checkpoint_path=None):\n",
        "    \"\"\"\n",
        "      Load model from latest checkpoint (get_latest_ckpt).\n",
        "    \"\"\"\n",
        "    assert checkpoint_dir or checkpoint_path\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        epoch, checkpoint_path = get_latest_ckpt(checkpoint_dir)\n",
        "    else:\n",
        "        epoch = int(checkpoint_path.split('.')[-2])\n",
        "\n",
        "    if checkpoint_path:\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "        print('Load from %s sucessful!' % checkpoint_path)\n",
        "        return model, epoch + 1\n",
        "    else:\n",
        "        return model, 0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzpjTRKbRQlT"
      },
      "source": [
        "class SeqModel(nn.Module):\n",
        "    \"\"\"\n",
        "      Sequential Model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, pad_id_src, pad_id_tgt, config=None):\n",
        "        super(SeqModel, self).__init__()\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.tgt_vocab_size = tgt_vocab_size\n",
        "        self.pad_id_src = pad_id_src\n",
        "        self.pad_id_tgt = pad_id_tgt\n",
        "        self.batch_size = config['data']['batch_size']\n",
        "        self.config = config\n",
        "        self.options = config['model']\n",
        "        self.model_type = config['model']['model_type']\n",
        "\n",
        "        self.src_embedding = nn.Embedding(self.src_vocab_size, self.options['emb_dim'], self.pad_id_src)\n",
        "\n",
        "        if self.config['data']['share_vocab']:\n",
        "            self.tgt_embedding = self.src_embedding\n",
        "        else:\n",
        "            self.tgt_embedding = nn.Embedding(\n",
        "                self.tgt_vocab_size,\n",
        "                self.options['emb_dim'],\n",
        "                self.pad_id_tgt)\n",
        "\n",
        "        if self.options['encoder'] == 'lstm':\n",
        "            self.encoder = Encoder(\n",
        "                self.options['emb_dim'],\n",
        "                self.options['src_hidden_dim'],\n",
        "                self.options['src_layers'],\n",
        "                self.options['bidirectional'],\n",
        "                self.options['dropout'])\n",
        "            self.ctx_bridge = nn.Linear(\n",
        "                self.options['src_hidden_dim'],\n",
        "                self.options['tgt_hidden_dim'])\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError('unknown encoder type')\n",
        "        \n",
        "        if self.model_type == 'delete':\n",
        "            self.attribute_embedding = nn.Embedding(num_embeddings=2, embedding_dim=self.options['emb_dim'])\n",
        "            attr_size = self.options['emb_dim']\n",
        "\n",
        "        elif self.model_type == 'delete_retrieve':\n",
        "            self.attribute_encoder = Encoder(\n",
        "                self.options['emb_dim'],\n",
        "                self.options['src_hidden_dim'],\n",
        "                self.options['src_layers'],\n",
        "                self.options['bidirectional'],\n",
        "                self.options['dropout'],\n",
        "                pack=False)\n",
        "            attr_size = self.options['src_hidden_dim']\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError('unknown model type')\n",
        "\n",
        "        self.c_bridge = nn.Linear(attr_size + self.options['src_hidden_dim'], self.options['tgt_hidden_dim'])\n",
        "        self.h_bridge = nn.Linear(attr_size + self.options['src_hidden_dim'], self.options['tgt_hidden_dim'])\n",
        "\n",
        "        self.decoder = StackedAttentionLSTM(config=config)\n",
        "\n",
        "        self.output_projection = nn.Linear(self.options['tgt_hidden_dim'], tgt_vocab_size)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"Initialize weights.\"\"\"\n",
        "        initrange = 0.1\n",
        "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.h_bridge.bias.data.fill_(0)\n",
        "        self.c_bridge.bias.data.fill_(0)\n",
        "        self.output_projection.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, input_src, input_tgt, srcmask, srclens, input_attr, attrlens, attrmask):\n",
        "        src_emb = self.src_embedding(input_src)\n",
        "\n",
        "        srcmask = (1-srcmask).byte()\n",
        "\n",
        "        src_outputs, (src_h_t, src_c_t) = self.encoder(src_emb, srclens, srcmask)\n",
        "\n",
        "        if self.options['bidirectional']:\n",
        "            h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1)\n",
        "            c_t = torch.cat((src_c_t[-1], src_c_t[-2]), 1)\n",
        "        else:\n",
        "            h_t = src_h_t[-1]\n",
        "            c_t = src_c_t[-1]\n",
        "\n",
        "        src_outputs = self.ctx_bridge(src_outputs)\n",
        "\n",
        "        if self.model_type == 'delete':\n",
        "            a_ht = self.attribute_embedding(input_attr)\n",
        "            c_t = torch.cat((c_t, a_ht), -1)\n",
        "            h_t = torch.cat((h_t, a_ht), -1)\n",
        "\n",
        "        elif self.model_type == 'delete_retrieve':\n",
        "            attr_emb = self.src_embedding(input_attr)\n",
        "            _, (a_ht, a_ct) = self.attribute_encoder(attr_emb, attrlens, attrmask)\n",
        "            if self.options['bidirectional']:\n",
        "                a_ht = torch.cat((a_ht[-1], a_ht[-2]), 1)\n",
        "                a_ct = torch.cat((a_ct[-1], a_ct[-2]), 1)\n",
        "\n",
        "            h_t = torch.cat((h_t, a_ht), -1)\n",
        "            c_t = torch.cat((c_t, a_ct), -1)\n",
        "            \n",
        "        c_t = self.c_bridge(c_t)\n",
        "        h_t = self.h_bridge(h_t)\n",
        "\n",
        "        tgt_emb = self.tgt_embedding(input_tgt)\n",
        "        tgt_outputs, (_, _) = self.decoder(tgt_emb, (h_t, c_t), src_outputs, srcmask)\n",
        "\n",
        "        tgt_outputs_reshape = tgt_outputs.contiguous().view(\n",
        "            tgt_outputs.size()[0] * tgt_outputs.size()[1],\n",
        "            tgt_outputs.size()[2])\n",
        "        decoder_logit = self.output_projection(tgt_outputs_reshape)\n",
        "        decoder_logit = decoder_logit.view(\n",
        "            tgt_outputs.size()[0],\n",
        "            tgt_outputs.size()[1],\n",
        "            decoder_logit.size()[1])\n",
        "\n",
        "        probs = self.softmax(decoder_logit)\n",
        "\n",
        "        return decoder_logit, probs\n",
        "\n",
        "    def count_params(self):\n",
        "        n_params = 0\n",
        "        for param in self.parameters():\n",
        "            n_params += np.prod(param.data.cpu().numpy().shape)\n",
        "        return n_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljVb0y_P-Q3k"
      },
      "source": [
        "# BLEU functions\n",
        "\n",
        "def bleu_stats(hypothesis, reference):\n",
        "    \"\"\"\n",
        "      Compute statistics for BLEU.\n",
        "    \"\"\"\n",
        "    \n",
        "    stats = []\n",
        "    stats.append(len(hypothesis))\n",
        "    stats.append(len(reference))\n",
        "    for n in range(1, 5):\n",
        "        s_ngrams = Counter(\n",
        "            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]\n",
        "        )\n",
        "        r_ngrams = Counter(\n",
        "            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]\n",
        "        )\n",
        "        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))\n",
        "        stats.append(max([len(hypothesis) + 1 - n, 0]))\n",
        "    return stats\n",
        "\n",
        "def bleu(stats):\n",
        "    \"\"\"\n",
        "      Compute BLEU given n-gram statistics.\n",
        "    \"\"\"\n",
        "    if len(list(filter(lambda x: x == 0, stats))) > 0:\n",
        "        return 0\n",
        "    (c, r) = stats[:2]\n",
        "    log_bleu_prec = sum([math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]) / 4.\n",
        "    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)\n",
        "\n",
        "def get_bleu(hypotheses, reference):\n",
        "    \"\"\"\n",
        "      Get validation BLEU score for dev set.\n",
        "    \"\"\"\n",
        "    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for hyp, ref in zip(hypotheses, reference):\n",
        "        stats += np.array(bleu_stats(hyp, ref))\n",
        "    return 100 * bleu(stats)\n",
        "\n",
        "def get_edit_distance(hypotheses, reference):\n",
        "    ed = 0\n",
        "    for hyp, ref in zip(hypotheses, reference):\n",
        "        ed += editdistance.eval(hyp, ref)\n",
        "\n",
        "    return ed * 1.0 / len(hypotheses)\n",
        "\n",
        "\n",
        "def decode_minibatch(max_len, start_id, model, src_input, srclens, srcmask, aux_input, auxlens, auxmask):\n",
        "    \"\"\" \n",
        "      Decoding minibatch\n",
        "    \"\"\"\n",
        "    # Initialize target with <s> for every sentence\n",
        "    tgt_input = Variable(torch.LongTensor([[start_id] for i in range(src_input.size(0))]))\n",
        "    if CUDA:\n",
        "        tgt_input = tgt_input.cuda()\n",
        "\n",
        "    for i in range(max_len):\n",
        "        # run input through the model\n",
        "        decoder_logit, word_probs = model(src_input, tgt_input, srcmask, srclens, aux_input, auxmask, auxlens)\n",
        "        decoder_argmax = word_probs.data.cpu().numpy().argmax(axis=-1)\n",
        "        # select the predicted \"next\" tokens, attach to target-side inputs\n",
        "        next_preds = Variable(torch.from_numpy(decoder_argmax[:, -1]))\n",
        "        if CUDA:\n",
        "            next_preds = next_preds.cuda()\n",
        "        tgt_input = torch.cat((tgt_input, next_preds.unsqueeze(1)), dim=1)\n",
        "\n",
        "    return tgt_input\n",
        "\n",
        "def decode_dataset(model, src, tgt, config):\n",
        "    \"\"\"\n",
        "      Evaluate model.\n",
        "    \"\"\"\n",
        "    inputs = []\n",
        "    preds = []\n",
        "    auxs = []\n",
        "    ground_truths = []\n",
        "\n",
        "    for j in range(0, len(src['data']), config['data']['batch_size']):\n",
        "        sys.stdout.write(\"\\r%s/%s...\" % (j, len(src['data'])))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # get batch\n",
        "        input_content, input_aux, output = minibatch(\n",
        "            src, tgt, j, \n",
        "            config['data']['batch_size'], \n",
        "            config['data']['max_len'], \n",
        "            config['model']['model_type'],\n",
        "            is_test=True)\n",
        "        input_lines_src, output_lines_src, srclens, srcmask, indices = input_content\n",
        "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
        "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
        "\n",
        "        tgt_pred = decode_minibatch(\n",
        "            config['data']['max_len'], tgt['tok2id']['<s>'], \n",
        "            model, input_lines_src, srclens, srcmask,\n",
        "            input_ids_aux, auxlens, auxmask)\n",
        "\n",
        "        # convert seqs to tokens\n",
        "        def ids_to_toks(tok_seqs, id2tok):\n",
        "            out = []\n",
        "            # take off the gpu\n",
        "            tok_seqs = tok_seqs.cpu().numpy()\n",
        "            # convert to toks, cut off at </s>, delete any start tokens (preds were kickstarted w them)\n",
        "            for line in tok_seqs:\n",
        "                toks = [id2tok[x] for x in line]\n",
        "                if '<s>' in toks: \n",
        "                    toks.remove('<s>')\n",
        "                cut_idx = toks.index('</s>') if '</s>' in toks else len(toks)\n",
        "                out.append( toks[:cut_idx] )\n",
        "            # unsort\n",
        "            out = unsort(out, indices)\n",
        "            return out\n",
        "\n",
        "        # convert inputs/preds/targets/aux to human-readable form\n",
        "        inputs += ids_to_toks(output_lines_src, src['id2tok'])\n",
        "        preds += ids_to_toks(tgt_pred, tgt['id2tok'])\n",
        "        ground_truths += ids_to_toks(output_lines_tgt, tgt['id2tok'])\n",
        "        \n",
        "        if config['model']['model_type'] == 'delete':\n",
        "            auxs += [[str(x)] for x in input_ids_aux.data.cpu().numpy()] \n",
        "        elif config['model']['model_type'] == 'delete_retrieve':\n",
        "            auxs += ids_to_toks(input_ids_aux, tgt['id2tok'])\n",
        "        elif config['model']['model_type'] == 'seq2seq':\n",
        "            auxs += ['None' for _ in range(len(tgt_pred))]\n",
        "\n",
        "    return inputs, preds, ground_truths, auxs\n",
        "\n",
        "\n",
        "def inference_metrics(model, src, tgt, config):\n",
        "    \"\"\" \n",
        "      Decode and evaluate BLEU scores. \n",
        "    \"\"\"\n",
        "\n",
        "    inputs, preds, ground_truths, auxs = decode_dataset(\n",
        "        model, src, tgt, config)\n",
        "\n",
        "    bleu = get_bleu(preds, ground_truths)\n",
        "    edit_distance = get_edit_distance(preds, ground_truths)\n",
        "\n",
        "    inputs = [' '.join(seq) for seq in inputs]\n",
        "    preds = [' '.join(seq) for seq in preds]\n",
        "    ground_truths = [' '.join(seq) for seq in ground_truths]\n",
        "    auxs = [' '.join(seq) for seq in auxs]\n",
        "\n",
        "    return bleu, edit_distance, inputs, preds, ground_truths, auxs\n",
        "\n",
        "\n",
        "def evaluate_lpp(model, src, tgt, config):\n",
        "    \"\"\" \n",
        "      Evaluate log perplexity WITHOUT decoding (i.e., with teacher forcing)\n",
        "    \"\"\"\n",
        "    \n",
        "    weight_mask = torch.ones(len(tgt['tok2id']))\n",
        "    if CUDA:\n",
        "        weight_mask = weight_mask.cuda()\n",
        "    weight_mask[tgt['tok2id']['<pad>']] = 0\n",
        "    loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)\n",
        "    if CUDA:\n",
        "        loss_criterion = loss_criterion.cuda()\n",
        "\n",
        "    losses = []\n",
        "    for j in range(0, len(src['data']), config['data']['batch_size']):\n",
        "        sys.stdout.write(\"\\r%s/%s...\" % (j, len(src['data'])))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # get batch\n",
        "        input_content, input_aux, output = minibatch(\n",
        "            src, tgt, j, \n",
        "            config['data']['batch_size'], \n",
        "            config['data']['max_len'], \n",
        "            config['model']['model_type'],\n",
        "            is_test=True)\n",
        "        input_lines_src, _, srclens, srcmask, _ = input_content\n",
        "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
        "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
        "\n",
        "        decoder_logit, decoder_probs = model(\n",
        "            input_lines_src, input_lines_tgt, srcmask, srclens,\n",
        "            input_ids_aux, auxlens, auxmask)\n",
        "\n",
        "        loss = loss_criterion(\n",
        "            decoder_logit.contiguous().view(-1, len(tgt['tok2id'])),\n",
        "            output_lines_tgt.view(-1)\n",
        "        )\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return np.mean(losses)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Ldsv02AQrR"
      },
      "source": [
        "Bleu = True\n",
        "overfit = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKJTKV5GBx36"
      },
      "source": [
        "# config file which has our fine-tuned model\n",
        "config = {\n",
        "  \"training\": {\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"learning_rate\": 0.0003,\n",
        "    \"max_norm\": 3.0,\n",
        "    \"epochs\": 45,\n",
        "    \"batches_per_report\": 200,\n",
        "    \"batches_per_sampling\": 500,\n",
        "    \"random_seed\": 1\n",
        "  },\n",
        "  \"data\": {\n",
        "    \"src\": \"/content/drive/MyDrive/data/yelp/sentiment.train.0\",\n",
        "    \"tgt\": \"/content/drive/MyDrive/data/yelp/sentiment.train.1\",\n",
        "    \"src_test\": \"/content/drive/MyDrive/data/yelp/reference.test.0\",\n",
        "    \"tgt_test\": \"/content/drive/MyDrive/data/yelp/reference.test.1\",\n",
        "    \"src_vocab\": \"/content/drive/MyDrive/data/yelp/vocab\",\n",
        "    \"tgt_vocab\": \"/content/drive/MyDrive/data/yelp/vocab\",\n",
        "    \"share_vocab\": True,\n",
        "    \"attribute_vocab\": \"/content/drive/MyDrive/data/yelp/ngram.15.attribute\",\n",
        "    \"ngram_attributes\": True,\n",
        "    \"batch_size\": 256,\n",
        "    \"max_len\": 50,\n",
        "    \"working_dir\": \"/content/drive/MyDrive/data/working_dir\"\n",
        "  },\n",
        "    \"model\": {\n",
        "        \"model_type\": \"delete\",\n",
        "        \"emb_dim\": 128,\n",
        "        \"attention\": False,\n",
        "        \"encoder\": \"lstm\",\n",
        "        \"src_hidden_dim\": 512,\n",
        "        \"src_layers\": 1,\n",
        "        \"bidirectional\": True,\n",
        "        \"tgt_hidden_dim\": 512,\n",
        "        \"tgt_layers\": 1,\n",
        "        \"decode\": \"greedy\",\n",
        "        \"dropout\": 0.2\n",
        "    }\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBmRDXRqgMl4"
      },
      "source": [
        "train_losses = []\n",
        "scores = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-axKdFtU7LXC"
      },
      "source": [
        "working_dir = config['data']['working_dir']\n",
        "\n",
        "if not os.path.exists(working_dir):\n",
        "    os.makedirs(working_dir)\n",
        "\n",
        "# set up logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    filename='%s/train_log' % working_dir,\n",
        ")\n",
        "\n",
        "console = logging.StreamHandler()\n",
        "console.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "console.setFormatter(formatter)\n",
        "logging.getLogger('').addHandler(console)\n",
        "\n",
        "logging.info('Reading data ...')\n",
        "src, tgt = read_nmt_data(\n",
        "    src=config['data']['src'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes']\n",
        ")\n",
        "\n",
        "src_test, tgt_test = read_nmt_data(\n",
        "    src=config['data']['src_test'],\n",
        "    config=config,\n",
        "    tgt=config['data']['tgt_test'],\n",
        "    attribute_vocab=config['data']['attribute_vocab'],\n",
        "    ngram_attributes=config['data']['ngram_attributes'],\n",
        "    train_src=src,\n",
        "    train_tgt=tgt\n",
        ")\n",
        "logging.info('...done!')\n",
        "\n",
        "\n",
        "batch_size = config['data']['batch_size']\n",
        "max_length = config['data']['max_len']\n",
        "src_vocab_size = len(src['tok2id'])\n",
        "tgt_vocab_size = len(tgt['tok2id'])\n",
        "\n",
        "\n",
        "weight_mask = torch.ones(tgt_vocab_size)\n",
        "weight_mask[tgt['tok2id']['<pad>']] = 0\n",
        "loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)\n",
        "if CUDA:\n",
        "    weight_mask = weight_mask.cuda()\n",
        "    loss_criterion = loss_criterion.cuda()\n",
        "\n",
        "torch.manual_seed(config['training']['random_seed'])\n",
        "np.random.seed(config['training']['random_seed'])\n",
        "\n",
        "model = SeqModel(\n",
        "    src_vocab_size=src_vocab_size,\n",
        "    tgt_vocab_size=tgt_vocab_size,\n",
        "    pad_id_src=src['tok2id']['<pad>'],\n",
        "    pad_id_tgt=tgt['tok2id']['<pad>'],\n",
        "    config=config\n",
        ")\n",
        "\n",
        "logging.info('MODEL HAS %s params' %  model.count_params())\n",
        "model, start_epoch = attempt_load_model(\n",
        "    model=model,\n",
        "    checkpoint_dir=working_dir)\n",
        "if CUDA:\n",
        "    model = model.cuda()\n",
        "\n",
        "\n",
        "if config['training']['optimizer'] == 'adam':\n",
        "    lr = config['training']['learning_rate']\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "elif config['training']['optimizer'] == 'sgd':\n",
        "    lr = config['training']['learning_rate']\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "elif config['training']['optimizer'] == 'adadelta':\n",
        "    lr = config['training']['learning_rate']\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
        "else:\n",
        "    raise NotImplementedError(\"Learning method not recommend for task\")\n",
        "\n",
        "epoch_loss = []\n",
        "start_since_last_report = time.time()\n",
        "\n",
        "words_since_last_report = 0\n",
        "losses_since_last_report = []\n",
        "best_metric = 0.0\n",
        "best_epoch = 0\n",
        "cur_metric = 0.0 # log perplexity or BLEU\n",
        "num_examples = min(len(src['content']), len(tgt['content']))\n",
        "num_batches = num_examples / batch_size\n",
        "\n",
        "STEP = 0\n",
        "for epoch in range(start_epoch, config['training']['epochs']):\n",
        "    if cur_metric > best_metric:\n",
        "        # delete old checkpoint to save memory\n",
        "        for ckpt_path in glob.glob(working_dir + '/model.*'):\n",
        "            os.system(\"rm %s\" % ckpt_path)\n",
        "        # replace with new checkpoint\n",
        "        torch.save(model.state_dict(), working_dir + '/model.%s.ckpt' % epoch)\n",
        "\n",
        "        best_metric = cur_metric\n",
        "        best_epoch = epoch - 1\n",
        "\n",
        "    losses = []\n",
        "    for i in range(0, num_examples, batch_size):\n",
        "\n",
        "        if overfit:\n",
        "            i = 50\n",
        "\n",
        "        batch_idx = i / batch_size\n",
        "\n",
        "        input_content, input_aux, output = minibatch(\n",
        "            src, tgt, i, batch_size, max_length, config['model']['model_type'])\n",
        "        input_lines_src, _, srclens, srcmask, _ = input_content\n",
        "        input_ids_aux, _, auxlens, auxmask, _ = input_aux\n",
        "        input_lines_tgt, output_lines_tgt, _, _, _ = output\n",
        "        \n",
        "        decoder_logit, decoder_probs = model(\n",
        "            input_lines_src, input_lines_tgt, srcmask, srclens,\n",
        "            input_ids_aux, auxlens, auxmask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_criterion(\n",
        "            decoder_logit.contiguous().view(-1, tgt_vocab_size),\n",
        "            output_lines_tgt.view(-1)\n",
        "        )\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        losses_since_last_report.append(loss.item())\n",
        "        epoch_loss.append(loss.item())\n",
        "        loss.backward()\n",
        "        norm = nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_norm'])\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if overfit or batch_idx % config['training']['batches_per_report'] == 0:\n",
        "\n",
        "            s = float(time.time() - start_since_last_report)\n",
        "            eps = (batch_size * config['training']['batches_per_report']) / s\n",
        "            avg_loss = np.mean(losses_since_last_report)\n",
        "            info = (epoch, batch_idx, num_batches, eps, avg_loss, cur_metric)\n",
        "            logging.info('EPOCH: %s ITER: %s/%s EPS: %.2f LOSS: %.4f METRIC: %.4f' % info)\n",
        "            start_since_last_report = time.time()\n",
        "            words_since_last_report = 0\n",
        "            losses_since_last_report = []\n",
        "\n",
        "        STEP += 1\n",
        "\n",
        "    logging.info('EPOCH %s COMPLETE. EVALUATING...' % epoch)\n",
        "    start = time.time()\n",
        "    model.eval()\n",
        "    dev_loss = evaluate_lpp(\n",
        "            model, src_test, tgt_test, config)\n",
        "\n",
        "    if Bleu and epoch >= config['training'].get('inference_start_epoch', 1):\n",
        "        cur_metric, edit_distance, _, preds, _, _ = inference_metrics(\n",
        "            model, src_test, tgt_test, config)\n",
        "\n",
        "        with open(working_dir + '/preds.%s' % epoch, 'w') as f:\n",
        "            f.write('\\n'.join(preds) + '\\n')\n",
        "\n",
        "\n",
        "    else:\n",
        "        cur_metric = dev_loss\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    logging.info('METRIC: %s. TIME: %.2fs CHECKPOINTING...' % (\n",
        "        cur_metric, (time.time() - start)))\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    train_losses.append(avg_loss)\n",
        "    scores.append(cur_metric)\n",
        "    epoch_loss = []\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-BsbkerzDhc"
      },
      "source": [
        "# Delete_retrieve model\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.title('DeleteAndRetrieve model -- train and dev losses')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-nup4nmzHfl"
      },
      "source": [
        "plt.plot(scores)\n",
        "plt.title('scores')\n",
        "plt.ylabel('DeleteAndRetrieve model -- scores')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqCtD-hrQWF_"
      },
      "source": [
        "#Delete model\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(dev_losses)\n",
        "plt.title(\"Delete model -- train and dev losses\")\n",
        "plt.ylabel('losses')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkSNFvl7g3fp"
      },
      "source": [
        "plt.plot(scores)\n",
        "plt.title('Delete model -- scores')\n",
        "plt.ylabel('scores')\n",
        "plt.xlaebl('epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAr9ocMZ6F2G"
      },
      "source": [
        "!python eval.py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}